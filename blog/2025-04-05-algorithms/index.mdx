---
slug: algorithms
title: Algorithms
authors: [nixoletas]
tags: [algorithms, programming basics]
image: /img/nick-logo.png
description: Remebering algorithms
keywords: ["algorithms", "programming basics"]
---

Algorithms are a fundamental part of programming. They are the building blocks of any program. It's a way to describe how to solve a problem.

<!-- truncate -->

## Sorting

---

### Bubble Sort

Pros ✅

1. **Easy to implement**:
   The algorithm is straightforward and ideal for teaching sorting concepts.

2. **In-place sorting**:
   Requires only a constant amount of additional memory $O(1)$.

3. **Stable sort**:
   Maintains the relative order of equal elements, which is useful in certain applications.

4. **Best case optimization possible**:
   With an early exit condition (when no swaps occur), it can perform in O(n) time for nearly sorted data.

Cons ❌

1. **Very inefficient for large datasets**:
   Has an average and worst-case time complexity of **O(n²)**, making it impractical for big lists.

2. **Too many swaps**:
   Performs more swaps than necessary compared to more efficient algorithms like insertion sort or quicksort.

3. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

4. **Rarely used in practice**:
   There are better-performing algorithms like Merge Sort, Quick Sort, and Tim Sort used in real-world applications.

:::info
Worst case: O(n²) | Best case: O(n)
:::
[Youtube - Bubble Sort by Michael Sambol](https://www.youtube.com/watch?v=xli_FI7CuzA)

```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n - 1):
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr

sorted_arr = bubble_sort([64, 34, 25, 12, 22, 11, 90])

print(sorted_arr)
```

### Insertion Sort

Pros ✅

1. **Simple and intuitive**:
   The algorithm is easy to understand and implement.

2. **Best case performance**:
   Can achieve O(n) time complexity for nearly sorted data.

3. **Stable sort**:
   Maintains the relative order of equal elements, which is useful in certain applications.

Cons ❌

1. **Very inefficient for large datasets**:
   Has an average and worst-case time complexity of **O(n²)**, making it impractical for big lists.

2. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

3. **Rarely used in practice**:
   There are better-performing algorithms like Merge Sort, Quick Sort, and Tim Sort used in real-world applications.

:::info
Worst case: O(n²) |Best case: O(n)
:::

[Youtube - Insertion Sort by Michael Sambol](https://www.youtube.com/watch?v=JU767SDMDvA)

```python
def insertion_sort(arr):
    for i in range(1, len(arr)):
        key = arr[i]
        j = i - 1
        while j >= 0 and key < arr[j]:
            arr[j + 1] = arr[j]
            j -= 1
        arr[j + 1] = key
    return arr

sorted_arr = insertion_sort([64, 34, 25, 12, 22, 11, 90])

print(sorted_arr)
```

### Merge Sort

Pros ✅

1. **Stable sort**:
   Maintains the relative order of equal elements, which is useful in certain applications.

2. **Best case performance**:
   Can achieve O(n) time complexity for nearly sorted data.

3. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

Cons ❌

1. **Not in-place sorting**:
   Requires additional memory proportional to the size of the input array.

2. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

:::info
Worst case: O(n log n) | Best case: O(n log n)
:::

[Youtube - Merge Sort Algorithm by Michael Sambol](https://www.youtube.com/watch?v=4VqmGXwpLqc)

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    while i < len(left) and j < len(right):
        if left[i] < right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    result.extend(left[i:])
    result.extend(right[j:])
    return result

sorted_arr = merge_sort([64, 34, 25, 12, 22, 11, 90])

print(sorted_arr)
```

### Quick Sort

Pros ✅

1. **Best case performance**:
   Can achieve O(n) time complexity for nearly sorted data.

2. **In-place sorting**:
   Requires only a constant amount of additional memory $O(1)$.

3. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

Cons ❌

1. **Not stable by default**:
   Equal elements may be reordered, which is not useful in certain applications.

2. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

:::info
Worst case: O(n²) | Best case: O(n log n)
:::

[Youtube - Quick Sort Algorithm by Michael Sambol](https://www.youtube.com/watch?v=Hoixgm4-P4M)

```python
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)

sorted_arr = quick_sort([64, 34, 25, 12, 22, 11, 90])

print(sorted_arr)
```

## Search

### Linear Search

Pros ✅

1. **Simple and intuitive**:
   The algorithm is easy to understand and implement.

2. **Best case performance**:
   Can achieve O(n) time complexity for nearly sorted data.

Cons ❌

1. **Very inefficient for large datasets**:
   Has an average and worst-case time complexity of **O(n²)**, making it impractical for big lists.

2. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

:::info
Worst case: O(n) | Best case: O(1)
:::

[Youtube - Linear Search Algorithm by Michael Sambol](https://www.youtube.com/watch?v=246V51AWwZM)

```python
def linear_search(arr, target):
    for i, value in enumerate(arr):
        if value == target:
            return i
    return -1

target = 25

index = linear_search([64, 34, 25, 12, 22, 11, 90], target)

print(index)
```

### Binary Search

Pros ✅

1. **Best case performance**:
   Can achieve O(n) time complexity for nearly sorted data.

2. **In-place sorting**:
   Requires only a constant amount of additional memory $O(1)$.

Cons ❌

1. **Not stable by default**:
   Equal elements may be reordered, which is not useful in certain applications.

2. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

:::info
Worst case: O(log n) | Best case: O(1)
:::

[Youtube - Binary Search by Michael Sambol](https://www.youtube.com/watch?v=fDKIpRe8GW4)

```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1

target = 25

index = binary_search([64, 34, 25, 12, 22, 11, 90], target)

print(index)
```

### Recursion

Pros ✅

1. **Simple and intuitive**:
   The algorithm is easy to understand and implement.

2. **Best case performance**:
   Can achieve O(n) time complexity for nearly sorted data.

Cons ❌

1. **Not stable by default**:
   Equal elements may be reordered, which is not useful in certain applications.

2. **Not adaptive by default**:
   Without optimization, it always goes through all iterations even if the list becomes sorted early.

:::info
Worst case: O(n) | Best case: O(1)
:::

[Youtube - Recursion in 100 seconds by Fireship](https://www.youtube.com/watch?v=rf60MejMz3E)

```python
def factorial(n):
    if n == 0:
        return 1
    return n * factorial(n - 1)

result = factorial(5)

print(result)
```

BFS (Breadth-First Search)

DFS (Depth-First Search)

Dijkstra
